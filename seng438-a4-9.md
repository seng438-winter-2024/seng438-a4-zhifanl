**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#4 – Mutation Testing and Web app testing**

| Group: 9   |
|-----------------|
| Student 1 Zhifan Li        |   
| Student 2 Sandip Mishra              |   
| Student 3 Shanzi Ye             |   
| Student 4 Fardin Aryan      

# Introduction

This lab report talks about looking into two main ways of testing: Mutation Testing and GUI Testing, and we used them on a specific website. In the first part, we explored Mutation Testing. We used a tool called PIT Mutations to add faults to Java code. Our job was to understand the mutation scores and look at which mutants were stopped or kept going. From this, we created new tests to make our testing better. The second part was about GUI Testing on a website we picked. We did this using a popular method called Selenium. This part let us learn how to use Selenium, a tool known for testing web interfaces, and see how it stacks up against others.

# Analysis of 10 Mutants of the Range class 

| Mutation ID | Line | Mutation Description | Detailed Analysis | Result |
|-------------|------|----------------------|-------------------|--------|
| 1 | 176 | Replaced boolean return with false | This mutation changes the intersects method so it always says `false’, pretending no ranges cross over, no matter where they are or how long they are. When this mutation is labled as "KILLED", it shows our testing can spot very small mistakes—even those that completely change what a method is supposed to do, like checking if ranges overlap. This makes sure our program can correctly find when ranges overlap, which is very important for features that rely on comparing ranges. | KILLED |
| 2 | 176 | Replaced boolean return with true | This mutation makes it so the method always returns `true’, wrongly indicating that all ranges overlap, no matter their actual positions. Our tests' ability to eliminate this mutation shows how strong our testing is in catching behaviors that are too forgiving, protecting against mistaken overlap findings. This accuracy is very important for programs where knowing if ranges overlap correctly is key to how they work or the results they give. | KILLED |
| 3 | 176 | Removed call to `getLowerBound` | Getting rid of this mutation shows our tests are really good at checking the basic parts of how overlaps are found, making sure the method looks at both ends of a range. This mutation shows why it's so important to test all the edges carefully in programs that deal with or compare ranges of data. | KILLED |
| 4 | 176 | Removed call to `getUpperBound` | This mutation tests the impact of ignoring the upper boundary in determining range intersections. The fact that it was killed signifies that our tests are designed to consider the complete span of a range, from start to end, validating the essential nature of the upper bound in accurate intersection logic. It highlights how we must think about every part of range data to keep things working right in tasks that compare ranges. | KILLED |
| 5 | 176 | Removed call to `intersects` | Removing the overlap check questions the main job of the intersects method. Our tests catching and then "killing" this mutation prove our tests are detailed, making sure essential tasks, like checking if ranges overlap, work well and stay protected. | KILLED |
| 6 | 176 | Replaced return of integer sized value with (x == 0 ? 1 : 0) | This mutation changes the method's logic by making it return different results based on certain conditions, which could mess up how it finds overlaps. When our tests "kill" this mutation, it shows how sharp our testing is at spotting changes that might not seem related but could make the method work wrong. This proves our tests look deep, not just at what the method is supposed to do but also at how it does it. | KILLED |
| 7| 189 | Negated conditional | This mutation flips the condition in the if statement of the constrain method. Normally, this method checks if a value is outside a range and needs adjusting. Changing the condition could make the method adjust values that don't need it, messing up how it works. The fact we "killed" this mutation shows our tests can make sure the method adjusts values only when it should, keeping it working right. | KILLED |
| 8 | 189 | Removed call to `contains` | Taking out the contains call skips checking if a value is in the range, which affects how the value is adjusted. If this change isn't caught ("killed"), it means our tests might not be good enough at making sure values inside the range aren't changed when they shouldn't be. This shows we need to make our tests better at checking this. | SURVIVED |
| 9 | 189 | Removed conditional - replaced equality check with false | Changing a check in the method to always be false means it won't fix values that are outside the range, turning off an important part of the constrain method. When we "kill" this mutation, it shows our tests are good at finding when the method doesn't correct values that should be brought back into the range, making sure the method does its job. | KILLED |
| 10 | 189 | Removed conditional - replaced equality check with true | Changing a condition in the method to always true might make it adjust values that are already fine, which isn't needed. If this mistake isn't caught, it means our tests missed checking for times the method changes values it shouldn't. This tells us we need better tests to catch these wrong changes. | SURVIVED |





# Report all the statistics and the mutation score for each test class

## Range.java(Oringinal) 


![method](https://github.com/seng438-winter-2024/seng438-a4-zhifanl/assets/114043815/fdf9fcf2-74b1-42ee-8a4d-26e7f191dae6)


## Range.java(Updated) 

![method](https://github.com/seng438-winter-2024/seng438-a4-zhifanl/assets/110203582/647e7cd8-1cc1-4e3c-b796-b8fde7d87851)


## DataUtilities.java(Original) 

![method](https://github.com/seng438-winter-2024/seng438-a4-zhifanl/assets/114043815/b8398d26-0350-44ba-b4ab-d2448e19594b)



## DataUtilities.java(Updated) 


![method](https://github.com/seng438-winter-2024/seng438-a4-zhifanl/assets/114043815/1e0a6e74-391a-4a6a-9132-6f76dab4f826)


# Analysis drawn on the effectiveness of each of the test classes
## 

Range.java: After we tested the file file from Assignmet 3, we got a 58 mutation score, which was not ideal. Then we added a bunch of test cases to kill the alive mutants. AFter that we increased the mutation score by 10%. The updated rangetest.java has a 68% mutation score.

DataUtilities.java: After we tested the file file from Assignmet 3, we got a 81 mutation score, which was already perfect. Then we added a bunch of test cases to kill the remaining alive mutants. AFter that we increased the mutation score by 11%. The updated rangetest.java has a 92% mutation score. 

# A discussion on the effect of equivalent mutants on mutation score accuracy

Mutation testing helps us find weaknesses in our tests by checking if they can catch changes in the code. However, some changes, called equivalent mutants, don't really change how the program works, which can trick us into thinking our tests are better or worse than they are. We learned it's important to look closely at mutants that our tests don't catch, to tell apart those that don't change the program from those missed because our tests aren't thorough enough. Understanding these tricky mutants is hard and needs deep knowledge about the code and what the mutants do. In this lab, we're working on making our tests better and avoiding wasting time on mutants that can't really be "killed."

# A discussion of what could have been done to improve the mutation score of the test suites

In our labs 2 and 3, we made tests that led to very good test suites for the Range and DataUtilities classes. For DataUtilities, we got up to 81% mutation coverage, but it was hard to improve this much because some mutants didn't change anything or weren't relevant. We looked closely at the details of these mutants and tried a new way to cover more. In the end, we managed to raise our score by 11%. We did something similar for the Range class. We started with a 58% score from testing and then checked which mutants were still "alive." After making new tests to target these, we increased our score by 10%. This approach could help us get even better scores in the future by finding and "killing" more mutants.

# Why do we need mutation testing? Advantages and disadvantages of mutation testing

Mutation testing is essential for evaluating the quality of our test suites and their effectiveness in identifying errors. By making small changes (mutations) to the software, we can test whether the current test suite can detect these changes, thereby assessing the suite's ability to catch errors.

## Advantages and Disadvantages of Mutation Testing

| Aspect                 | Advantages                                                                                     | Disadvantages                                                                                  |
|------------------------|------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|
| **Automation**         | Mutation testing is automated, reducing the need for manual intervention.                      | The process can be time-intensive, potentially slowing down the development process.           |
| **Quality Assessment** | Provides a mutation score, indicating test suite quality and when to cease testing.           | The presence of equivalent mutants, which the test suite cannot detect, poses a challenge.     |
| **Error Identification** | Enhances the ability to identify hidden errors by testing the test suite's error detection capabilities. | Requires significant computational resources, which may not be feasible for all projects. |
| **Test Suite Improvement** | Encourages the improvement of the test suite's coverage and effectiveness.                   | May produce false positives, requiring additional analysis to identify valid issues.          |











# Explain your SELENUIM test case design process
## Shanzi Ye
Shanzi tested two main features on the eBay website: Providing Feedback and Getting Help. These tests simulate common user interactions with the site, verifying the functionality and user experience for feedback submission and help information retrieval.

| Feature              | Description                                                   |
|----------------------|---------------------------------------------------------------|
| Providing Feedback   | Tests the user's ability to provide feedback through the eBay website's feedback form. This includes opening the feedback form, selecting a rating, entering feedback text, and submitting the feedback. |
| Getting Help         | Tests the user's ability to search for help topics through the eBay Help and Contact page. This includes navigating to the Help page, entering search terms, executing the search, and verifying the results. |

## Zhifan Li
Zhifan conducted Selenium automated tests on the eBay website, focusing on user authentication, search functionality, and account management. These tests are designed to simulate common user interactions, such as logging in with valid credentials, searching for items using specific keywords, viewing item details, and logging out. 

| Feature                                         | Description                                                                                                 |
|-------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| Login With  Credentials                    | Tests the user's ability to log in using valid credentials, including navigating to the sign-in page and entering user details. |
| Search With Keywords and View First Item  | Extends the search test by selecting and viewing the first item in the search results, verifying item details. |


## Sandip Mishra

Sandip tested the search bar functionality in various scenarios and the language change feature. These tests simulate user interactions to verify the search functionality with different inputs and the ability to change the website's language setting.

| Feature                             | Description                                                                |
|-------------------------------------|----------------------------------------------------------------------------|
| Search Bar Functionality            | Tests the search bar with different keywords, including large queries and no queries, ensuring accurate results are displayed. |
| Language Change Feature             | Verifies the functionality of changing the site's language setting, ensuring the site remains functional after language switches. |


## Fardin Aryan

Fardin focused on features that enhance user navigation and access: shopping by category and changing the country settings. 

| Feature                               | Description                                                                 |
|---------------------------------------|-----------------------------------------------------------------------------|
| Shop by Category with Subcategory     | Verifies the user's ability to navigate eBay’s inventory by selecting various categories and subcategories. |
| Change Country to Different Country   | Tests the functionality of the eBay site to switch to a different country version, checking for appropriate content changes. |




# Explain the use of assertions and checkpoints

In our tests on eBay, we use checks to make sure things like searching for products and changing languages work right. For example, after searching for "ipad," we check to see if "ipad" is mentioned in the search results, showing the search worked. Also, when we change the site to Taiwan, we look for the phrase "支援及聯絡" (Support and Contact) to confirm the language switched correctly. These steps are crucial for making sure every part of our test does what it's supposed to, ensuring the website works as it should.

# how did you test each functionaity with different test data

## Shanzi Ye
| Test Functionality             | Scenario                                   | Description |
|--------------------------------|---------------------------------------------|-------------|
| Providing Feedback             | Positive feedback text                      | Tests the functionality of the feedback form by entering positive text to simulate a satisfied user experience. |
| Providing Feedback             | No feedback text (empty parameters)         | Verifies that the feedback form handles empty submissions correctly and provides appropriate feedback to the user. |
| Providing Feedback             | All empty parameters                        | Checks the behavior of the feedback submission when all fields are left empty, ensuring error handling and validation work. |
| Getting Help with Specifics    | Query with specific keywords ("Stolen Items") | Assesses the help section's search functionality by looking up terms related to specific issues, confirming the relevancy of search results. |
| Getting Help without Parameters| No search query                             | Tests the default state and response of the help section when no query is entered. |
| Getting Help with Random Text  | Random alphanumeric string                  | Evaluates the search function's handling of random, irrelevant input, testing the system's robustness against unexpected or non-standard queries. |

## Zhifan Li
| Functionality                             | Scenario                                  | Description |
|-------------------------------------------|-------------------------------------------|-------------|
| Login With Valid Credentials              | Normal login process                      | Validates the user's ability to log in with valid credentials using eBay's standard login flow, including optional sign-in with Google. |
| Login and Skip Additional Authentication  | Skipping optional authentication steps    | Tests the login flow when the user opts to skip additional authentication options like Google sign-in. |
| Verify Successful Login                   | Confirming user identity after login      | Ensures that after login, the system correctly displays the user's identity, confirming a successful sign-in. |
| Search With Valid Keywords                | Performing a search with common keywords  | Verifies the search functionality by entering common search terms and assessing the presence of search results. |
| Viewing First Item From Search Results    | Detailed view of a search result          | After a search, tests the functionality of viewing detailed information for the first item listed in the search results. |
| Verify Listing Details                    | Confirming details of a listed item       | Ensures that the details of a listed item, such as title and options, are present and correct. |
| Logout With Valid Credentials             | Normal logout process                     | Confirms the user's ability to log out properly and verifies the correct response of the website upon logout. |

## Sandip Mishra
| Functionality                         | Scenario                                          | Description |
|---------------------------------------|---------------------------------------------------|-------------|
| Search Bar Functionality              | Searching for a specific item (iPad)              | Verifies the search bar's ability to find items using a specific keyword and checks if the results are relevant. |
| Search with Large Query               | Attempting a search with an extensive query       | Tests the search bar's response to a large search query and whether it handles it gracefully without showing results. |
| Search with No Query                  | Triggering a search without entering any keywords | Examines how the search functionality behaves when no search terms are provided and ensures no irrelevant results are shown. |
| Language Change Interface             | Switching the interface language to French        | Confirms that the language change functionality works correctly and that the interface updates to reflect the new language choice. |
| Language Change, Search, and Revert   | Performing a search after a language change       | Checks if the search feature works correctly after the interface language has been changed and then reverts the language to English to validate consistency. |


## Fardin Aryan
| Functionality                              | Scenario                                  | Description |
|--------------------------------------------|-------------------------------------------|-------------|
| Shop by Category with Subcategory          | Navigating to a subcategory (Antiques)    | Confirms the site's navigation through categories and subcategories, and verifies the page title and breadcrumb trail for accuracy. |
| Shop by Category with Main Category        | Browsing a main category (Collectibles & Art) | Checks the functionality of shopping by main categories and ensures that relevant page titles and category descriptions appear. |
| Change Country to Different Country        | Changing the site's country to Taiwan     | Tests the ability to switch to a different country's version of the site and checks for the corresponding language and support link updates. |
| Change Country to the Same Country         | Selecting the same country (Canada)       | Ensures the site does not redirect or change unnecessarily when selecting the country version that is already active. |

# Discuss advantages and disadvantages of Selenium vs. Sikulix

Both Selenium and SikuliX are popular tools in the automation testing landscape, each offering unique advantages and facing certain limitations. Below is a comparison to help understand their strengths and weaknesses.

## Selenium

Selenium is widely used for automating web browsers, providing a robust framework for testing web applications across different browsers and platforms.

### Advantages and Disadvantages

| Aspect             | Advantages                                                                 | Disadvantages                                                 |
|--------------------|----------------------------------------------------------------------------|---------------------------------------------------------------|
| **Support**        | Extensive cross-platform and cross-browser support.                        | Limited to the testing of web applications only.              |
| **Versatility**    | Highly versatile for web testing scenarios.                               |                                                               |
| **Community**      | Large community support and extensive documentation.                       |                                                               |

## SikuliX

SikuliX leverages visual recognition to automate actions on both web and desktop applications, making it useful for a variety of testing scenarios beyond traditional web testing.

### Advantages and Disadvantages

| Aspect                 | Advantages                                                                                  | Disadvantages                                                        |
|------------------------|---------------------------------------------------------------------------------------------|----------------------------------------------------------------------|
| **Capability**         | Can automate Flash objects, providing a unique advantage.                                   | Faces challenges with cross-browser testing.                        |
| **Utility**            | Extends automation capabilities to desktop applications, broadening its utility.            |                                                                      |
| **Versatility**        | Useful for scenarios where traditional object identification fails.                         |                                                                      |

## Conclusion

Selenium and SikuliX both have their own special features for testing. Selenium is great for testing websites on different browsers and systems. SikuliX, however, is good for working with Flash and desktop apps, so it can test more than just websites. But when deciding which one to use, it really depends on what you need for your testing project because each tool has its own pros and cons.

# How the team work/effort was divided and managed
For our project, we made sure everyone on our team had a fair amount of work and we all worked together closely. Everyone played a big part, making sure the work was evenly spread out. Here's a closer look at how we divided up the tasks:


## Work Distribution

### Part 1: Enhancing Mutation Scores and Reporting

- **Group 1: Shanzi and Fardin**: Tasked with improving the mutation score of `RangeTest.java`. Group 1's responsibilities also include drafting the associated report, detailing the strategies employed and the outcomes achieved.

- **Group2: Zhifan and Sandip**: Focused on elevating the mutation score of `DataUtility.java`. Similar to their group 1, group 2 are also in charge of composing the report related to their task, encompassing both the methodology used and the results obtained.

### Part 2: Case Testing Requirements

- As per assignment requirements, each team member was required to test at least two cases. Adhering to this guideline, everyone successfully executed two case tests, demonstrating our collective commitment to fulfilling the project requirements comprehensively.





# Difficulties encountered, challenges overcome, and lessons learned

Working on Lab 4, we ran into a bunch of problems, especially with understanding what we needed to do and setting up the PITest environment. The lab instructions weren't clear, which confused us and made everything take longer. We also had trouble with PITest itself – it didn't always work right and we had to restart it many times. These issues ate up a lot of our time, slowing us down and making it tough to get a grip on the lab tasks. Despite these hurdles, we managed to complete the assignment by working together and sticking with it. Through this experience, we learned how important it is to keep trying and to work as a team, and it also made us better at solving technical problems.






# Comments/feedback on the lab itself

Our skills in software testing got better thanks to our work on the Web App and Mutation Testing Lab. In the beginning, it was a bit hard because the instructions weren't very detailed, but using PIT Mutation and Selenium ended up being really helpful. The highlight was working together as a team to get through the tough parts. We liked the challenge, even when it was hard to figure things out. For the future, having more examples, better support at the start, and clearer instructions would make the lab even better for learning.
